---

# ğŸ“˜ Mathematical Foundations for Machine Learning

**Read Time:** ~12â€“15 minutes

This lecture revisits core mathematical concepts from childhoodâ€”**vectors, matrices, calculus, and probability**â€”and explains **why they are critical for Machine Learning (ML)**. These are the building blocks behind algorithms like **linear regression, neural networks, PCA, and backpropagation**.

---

## 1ï¸âƒ£ VECTORS

### ğŸ”¹ What is a Vector?

A **vector** is an ordered list of numbers.

* In **physics**: magnitude + direction
* In **math / ML**: features or data points

In data science, **each data point = a vector**

### ğŸ”¹ Types of Vectors

**Row Vector**

```
[2  3]
```

**Column Vector**

```
[2]
[3]
```

---

### ğŸ”¹ Example (ML Context)

Suppose we predict exam scores using:

* Study hours
* Sleep hours

Then one student can be represented as:

```
x = (2, 3)
```

Where:

* 2 = study hours
* 3 = sleep hours

This vector lives in **2-dimensional space (RÂ²)**.

---

### ğŸ”¹ Vector Addition

Add corresponding elements:

```
A = (2, 3)
B = (1, 4)

A + B = (2+1, 3+4) = (3, 7)
```

âœ” Used when combining features or signals.

---

### ğŸ”¹ Scalar Multiplication

Multiply vector by a number:

```
2 Ã— (2, 3) = (4, 6)
```

âœ” Changes **magnitude**, not direction
âœ” Used in **weight scaling**

---

## 2ï¸âƒ£ MATRICES

### ğŸ”¹ What is a Matrix?

A **matrix** is a collection of vectors arranged in rows and columns.

```
A = [1  2]
    [3  4]
```

* Rows = data points
* Columns = features

---

### ğŸ”¹ Matrix Dimensions

Written as:

```
rows Ã— columns
```

Example:

```
2 Ã— 2 matrix
```

---

### ğŸ”¹ Matrix Multiplication

**Rule:**

> Columns of first matrix = Rows of second matrix

#### Example

```
A = [1  2]   (2Ã—2)
    [3  4]

B = [5]
    [6]     (2Ã—1)
```

Result:

```
A Ã— B = [1Ã—5 + 2Ã—6]
        [3Ã—5 + 4Ã—6]

      = [17]
        [39]
```

âœ” Used heavily in:

* Neural networks
* Linear regression
* Image processing

---

## 3ï¸âƒ£ DOT PRODUCT

### ğŸ”¹ What is Dot Product?

Dot product measures **how similar two vectors are**.

Formula:

```
A Â· B = Aâ‚Bâ‚ + Aâ‚‚Bâ‚‚ + ... + Aâ‚™Bâ‚™
```

---

### ğŸ”¹ Example Calculation

```
A = (2, 3)
B = (4, 1)

A Â· B = (2Ã—4) + (3Ã—1) = 8 + 3 = 11
```

---

### ğŸ”¹ Interpretation

| Dot Product | Meaning                     |
| ----------- | --------------------------- |
| Positive    | Similar direction           |
| Zero        | Perpendicular (no relation) |
| Negative    | Opposite direction          |

---

### ğŸ”¹ Cosine Similarity

Formula:

```
cos(Î¸) = (A Â· B) / (||A|| ||B||)
```

Range:

```
-1 to +1
```

âœ” Used in:

* Recommendation systems
* Search engines
* Vector databases (FAISS, Pinecone)

---

## 4ï¸âƒ£ EIGENVALUES & EIGENVECTORS

### ğŸ”¹ Intuition

When a matrix transforms a vector:

```
A Ã— v = Î» Ã— v
```

* **v** = eigenvector (direction unchanged)
* **Î»** = eigenvalue (scaling factor)

---

### ğŸ”¹ Real-Life Analogy

A **spring**:

* Can stretch or compress
* Direction stays same
* Length changes

---

### ğŸ”¹ Example Use: PCA

**Principal Component Analysis (PCA)**:

* Finds directions (eigenvectors)
* With maximum variance
* Reduces dimensions **without losing meaning**

âœ” Used in:

* Dimensionality reduction
* Image compression
* Noise removal

âœ” Also used in **Google PageRank**

---

## 5ï¸âƒ£ DIFFERENTIAL CALCULUS

### ğŸ”¹ What is Differentiation?

Differentiation measures **change**.

```
dy/dx = rate of change of y w.r.t x
```

---

### ğŸ”¹ Example

```
y = 2x
```

Derivative:

```
dy/dx = 2
```

Meaning:

> If x increases by 1 â†’ y increases by 2

---

### ğŸ”¹ Graph Meaning

* dy/dx = slope of tangent
* Constant slope â†’ straight line
* Changing slope â†’ curve

âœ” Used in:

* Loss minimization
* Optimization

---

## 6ï¸âƒ£ GRADIENT (MULTIVARIABLE CALCULUS)

### ğŸ”¹ Why Gradient?

In ML:

* Output depends on many variables

Example:

```
z = xÂ² + yÂ²
```

---

### ğŸ”¹ Partial Derivatives

```
âˆ‚z/âˆ‚x = 2x
âˆ‚z/âˆ‚y = 2y
```

Gradient vector:

```
âˆ‡z = (2x, 2y)
```

âœ” Gradient points in **steepest increase direction**

âœ” Used in:

* Gradient Descent
* Neural networks

---

## 7ï¸âƒ£ CHAIN RULE

### ğŸ”¹ What is Chain Rule?

Used when **functions are nested**.

```
y = f(g(x))
```

Derivative:

```
dy/dx = (dy/dg) Ã— (dg/dx)
```

---

### ğŸ”¹ Example

```
y = (3x + 1)Â²
```

Let:

```
g(x) = 3x + 1
y = gÂ²
```

Derivatives:

```
dy/dg = 2g
dg/dx = 3
```

Final:

```
dy/dx = 2(3x + 1) Ã— 3 = 6(3x + 1)
```

---

### ğŸ”¹ ML Importance

âœ” Core of **Backpropagation**
âœ” Used to update weights layer-by-layer
âœ” Backbone of deep learning

---

## ğŸ”š Conclusion

These concepts form the **mathematical spine of Machine Learning**:

| Concept     | Used In              |
| ----------- | -------------------- |
| Vectors     | Features, embeddings |
| Matrices    | Neural networks      |
| Dot Product | Similarity           |
| Eigen       | PCA                  |
| Derivatives | Optimization         |
| Gradient    | Learning             |
| Chain Rule  | Backpropagation      |

ğŸ“Œ **Next Topics Coming Up**

* Linear Regression
* Loss Functions
* Gradient Descent
* Backward Propagation (Deep Dive)

---

